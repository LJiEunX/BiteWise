# -*- coding: utf-8 -*-
"""Restaurant Recommendation DL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JcdT8SCVVX4iZCA40t9ki-9hHMbUhvmQ
"""

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, GlobalMaxPooling1D
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

"""# New section"""

# Load the dataset
file_path = 'caps_fin - clean.csv'
data = pd.read_csv(file_path)

# Display the first few rows of the dataset
data.head()

plt.subplots(figsize=(8, 8))
df_2dhist = pd.DataFrame({
    x_label: grp['type'].value_counts()
    for x_label, grp in data.groupby('area')
})
sns.heatmap(df_2dhist, cmap='viridis')
plt.xlabel('area')
_ = plt.ylabel('type')

data['average_price'].plot(kind='hist', bins=20, title='average_price')
plt.gca().spines[['top', 'right',]].set_visible(False)

data.groupby('type').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

data.groupby('area').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

data['opening_hours'].plot(kind='hist', bins=20, title='opening_hours')
plt.gca().spines[['top', 'right',]].set_visible(False)

# Checking for missing values
data.isnull().sum()

# Dropping rows with missing values
data_cleaned = data.dropna()

# Normalizing text data: converting to lowercase and removing special characters
data_cleaned['resto_name'] = data_cleaned['resto_name'].str.lower().str.replace('[^a-zA-Z0-9 ]', '')
data_cleaned['address'] = data_cleaned['address'].str.lower().str.replace('[^a-zA-Z0-9 ]', '')
data_cleaned['area'] = data_cleaned['area'].str.lower().str.replace('[^a-zA-Z0-9 ]', '')
data_cleaned['type'] = data_cleaned['type'].str.lower().str.replace('[^a-zA-Z0-9 ]', '')

# Display the cleaned data
data_cleaned.head()

# Concatenating relevant text features for the content-based filtering
data_cleaned['text'] = data_cleaned['resto_name'] + ' ' + data_cleaned['address'] + ' ' + data_cleaned['area'] + ' ' + data_cleaned['type']

# Tokenizing the text data
tokenizer = Tokenizer()
tokenizer.fit_on_texts(data_cleaned['text'])
sequences = tokenizer.texts_to_sequences(data_cleaned['text'])
word_index = tokenizer.word_index

# Padding the sequences
max_sequence_length = 100
data_padded = pad_sequences(sequences, maxlen=max_sequence_length)

# Creating labels (ratings)
labels = data_cleaned['rating'].values

# Splitting the data into training and validation sets
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(data_padded, labels, test_size=0.2, random_state=42)

# Defining the model
input_layer = Input(shape=(max_sequence_length,))
embedding_layer = Embedding(len(word_index) + 1, 128, input_length=max_sequence_length)(input_layer)
x = LSTM(64, return_sequences=True)(embedding_layer)
x = GlobalMaxPooling1D()(x)
x = Dense(64, activation='relu')(x)
output_layer = Dense(1, activation='linear')(x)

model = Model(input_layer, output_layer)
model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])

# Training the model
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val))

# Evaluating the model on the validation set
val_loss, val_accuracy = model.evaluate(X_val, y_val)

# Displaying the evaluation results
val_loss, val_accuracy

# Saving the model to an H5 file
model.save('restaurant_recommendation_model.h5')

